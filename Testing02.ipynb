{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nFhahkFoTCY",
        "outputId": "29be6091-9989-43bc-fe68-3fc382f9d2a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn xgboost pydantic\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix hardcoded Windows path in inference_pipeline.py\n",
        "with open(\"inference_pipeline.py\", \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(\"inference_pipeline.py\", \"w\") as f:\n",
        "    for line in lines:\n",
        "        if \"joblib.load\" in line and \"best_xgboost_model.pkl\" in line:\n",
        "            f.write(\"        model = joblib.load('best_xgboost_model.pkl')\\n\")\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n"
      ],
      "metadata": {
        "id": "FDV7DIzcovAs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Fully overwrite load_model function to avoid hardcoded path issue\n",
        "with open(\"inference_pipeline.py\", \"r\") as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Replace entire load_model() function with Colab-safe version\n",
        "updated_content = content.replace(\n",
        "    \"def load_model():\",\n",
        "    \"\"\"def load_model():\n",
        "    import joblib\n",
        "    import logging\n",
        "    try:\n",
        "        model = joblib.load(\"best_xgboost_model.pkl\")\n",
        "        logging.info(\"‚úÖ Model loaded successfully\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Failed to load model: {e}\")\n",
        "        return None\"\"\"\n",
        ")\n",
        "\n",
        "# Save it back\n",
        "with open(\"inference_pipeline.py\", \"w\") as f:\n",
        "    f.write(updated_content)\n"
      ],
      "metadata": {
        "id": "gBIqHP85pf_L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîÅ Rewrite the full load_model function with hard path replaced\n",
        "def patch_model_loader():\n",
        "    with open(\"inference_pipeline.py\", \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    new_lines = []\n",
        "    inside_load_model = False\n",
        "\n",
        "    for line in lines:\n",
        "        if \"def load_model\" in line:\n",
        "            inside_load_model = True\n",
        "            new_lines.append(\"def load_model():\\n\")\n",
        "            new_lines.append(\"    import joblib\\n\")\n",
        "            new_lines.append(\"    import logging\\n\")\n",
        "            new_lines.append(\"    try:\\n\")\n",
        "            new_lines.append(\"        model = joblib.load('best_xgboost_model.pkl')\\n\")\n",
        "            new_lines.append(\"        logging.info('‚úÖ Model loaded successfully')\\n\")\n",
        "            new_lines.append(\"        return model\\n\")\n",
        "            new_lines.append(\"    except Exception as e:\\n\")\n",
        "            new_lines.append(\"        logging.error(f'‚ùå Failed to load model: {e}')\\n\")\n",
        "            new_lines.append(\"        return None\\n\")\n",
        "        elif inside_load_model:\n",
        "            # skip original lines inside old load_model()\n",
        "            if line.strip() == \"\":\n",
        "                inside_load_model = False\n",
        "        else:\n",
        "            new_lines.append(line)\n",
        "\n",
        "    with open(\"inference_pipeline.py\", \"w\") as f:\n",
        "        f.writelines(new_lines)\n",
        "\n",
        "patch_model_loader()\n"
      ],
      "metadata": {
        "id": "HwFg4I1Hpzam"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = \"\"\"\n",
        "import logging\n",
        "import joblib\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "def load_model():\n",
        "    try:\n",
        "        model = joblib.load(\"best_xgboost_model.pkl\")\n",
        "        logging.info(\"‚úÖ Model loaded successfully\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        logging.error(f\"‚ùå Failed to load model: {e}\")\n",
        "        return None\n",
        "\n",
        "def preprocess_data(data: dict):\n",
        "    # Dummy feature vector with 50 features\n",
        "    features = list(range(50))\n",
        "    logging.debug(f\"[DEBUG] Feature count: {len(features)}\")\n",
        "    return np.array([features])\n",
        "\n",
        "def predict(model, features):\n",
        "    return model.predict(features).tolist()\n",
        "\n",
        "def validate_input(data: dict):\n",
        "    required_keys = [\"shipment_id\", \"origin\", \"destination\", \"distance_km\", \"shipment_type\", \"weight_kg\", \"creation_time\"]\n",
        "    for key in required_keys:\n",
        "        if key not in data:\n",
        "            return False, f\"Missing required field: {key}\"\n",
        "    return True, None\n",
        "\n",
        "def predict_delivery_time(input_data: dict):\n",
        "    model = load_model()\n",
        "    if not model:\n",
        "        return {\"success\": False, \"error\": \"Model not loaded\", \"predictions\": None}\n",
        "\n",
        "    is_valid, error = validate_input(input_data)\n",
        "    if not is_valid:\n",
        "        return {\"success\": False, \"error\": error, \"predictions\": None}\n",
        "\n",
        "    features = preprocess_data(input_data)\n",
        "    preds = predict(model, features)\n",
        "    return {\"success\": True, \"error\": None, \"predictions\": preds}\n",
        "\n",
        "# Optional test\n",
        "if __name__ == \"__main__\":\n",
        "    test_data = {\n",
        "        \"shipment_id\": \"SHIP12345\",\n",
        "        \"origin\": \"Warehouse A\",\n",
        "        \"destination\": \"City B\",\n",
        "        \"distance_km\": 250.0,\n",
        "        \"shipment_type\": \"Standard\",\n",
        "        \"weight_kg\": 20.5,\n",
        "        \"creation_time\": \"2023-07-27 10:30:00\"\n",
        "    }\n",
        "    print(predict_delivery_time(test_data))\n",
        "\"\"\"\n",
        "\n",
        "# Overwrite the file completely\n",
        "with open(\"inference_pipeline.py\", \"w\") as f:\n",
        "    f.write(code)\n",
        "\n",
        "print(\"‚úÖ inference_pipeline.py has been rewritten cleanly.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5umghugqIqR",
        "outputId": "04480aac-1eb1-4725-cd8e-15bdaefd6d0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ inference_pipeline.py has been rewritten cleanly.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload\n",
        "import inference_pipeline\n",
        "reload(inference_pipeline)\n",
        "\n",
        "from inference_pipeline import predict_delivery_time\n",
        "\n",
        "sample_input = {\n",
        "    \"shipment_id\": \"SHIP12345\",\n",
        "    \"origin\": \"Warehouse A\",\n",
        "    \"destination\": \"City B\",\n",
        "    \"distance_km\": 250.0,\n",
        "    \"shipment_type\": \"Standard\",\n",
        "    \"weight_kg\": 20.5,\n",
        "    \"creation_time\": \"2023-07-27 10:30:00\"\n",
        "}\n",
        "\n",
        "prediction = predict_delivery_time(sample_input)\n",
        "print(\"‚úÖ Prediction result:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFOfNmbkqLxd",
        "outputId": "2f8b8fd7-05ed-41ab-c88f-43c5455d553f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Prediction result: {'success': True, 'error': None, 'predictions': [39.69167709350586]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from api_app import app\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "# ‚úÖ Sample input for testing\n",
        "sample_input = {\n",
        "    \"shipment_id\": \"SHIP12345\",\n",
        "    \"origin\": \"Warehouse A\",\n",
        "    \"destination\": \"City B\",\n",
        "    \"distance_km\": 250.0,\n",
        "    \"shipment_type\": \"Standard\",\n",
        "    \"weight_kg\": 20.5,\n",
        "    \"creation_time\": \"2023-07-27 10:30:00\"\n",
        "}\n",
        "\n",
        "# üîé Test /health endpoint\n",
        "res = client.get(\"/health\")\n",
        "print(\"‚úÖ /health:\", res.status_code, res.json())\n",
        "\n",
        "# üîé Test /predict endpoint\n",
        "res = client.post(\"/predict\", json=sample_input)\n",
        "print(\"‚úÖ /predict:\", res.status_code, res.json())\n",
        "\n",
        "# üîé Test /predict/batch endpoint\n",
        "batch_input = [sample_input for _ in range(5)]\n",
        "res = client.post(\"/predict/batch\", json=batch_input)\n",
        "print(\"‚úÖ /predict/batch:\", res.status_code, res.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frSxj5j3qhf-",
        "outputId": "d1ffe1dd-1b4a-45ca-db26-871769f64a09"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ /health: 200 {'status': 'unhealthy', 'timestamp': '2025-07-27T16:14:37.420319', 'model_loaded': False, 'uptime_seconds': 0.3915891647338867}\n",
            "‚úÖ /predict: 503 {'success': False, 'error': 'Model not loaded. Please check service status.', 'request_id': 'req_1753632877428', 'timestamp': '2025-07-27T16:14:37.428602'}\n",
            "‚úÖ /predict/batch: 503 {'success': False, 'error': 'Model not loaded. Please check service status.', 'request_id': 'req_1753632877434', 'timestamp': '2025-07-27T16:14:37.434196'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_app_code = '''\n",
        "from fastapi import FastAPI, Request\n",
        "from inference_pipeline import load_model, predict_delivery_time\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "app = FastAPI()\n",
        "model = None  # Will be loaded at startup\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "def load_model_once():\n",
        "    global model\n",
        "    model = load_model()\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health_check():\n",
        "    global model\n",
        "    return {\n",
        "        \"status\": \"ok\" if model else \"error\",\n",
        "        \"message\": \"Model loaded and ready\" if model else \"Model not loaded\"\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict_endpoint(request: Request, input_data: dict):\n",
        "    global model\n",
        "    if model is None:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": \"Model not loaded. Please check service status.\",\n",
        "            \"request_id\": f\"req_{int(datetime.utcnow().timestamp() * 1000)}\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "\n",
        "    result = predict_delivery_time(input_data)\n",
        "    return {\n",
        "        **result,\n",
        "        \"request_id\": f\"req_{int(datetime.utcnow().timestamp() * 1000)}\",\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict/batch\")\n",
        "def predict_batch_endpoint(request: Request, input_data_list: list):\n",
        "    global model\n",
        "    if model is None:\n",
        "        return {\n",
        "            \"success\": False,\n",
        "            \"error\": \"Model not loaded. Please check service status.\",\n",
        "            \"request_id\": f\"req_{int(datetime.utcnow().timestamp() * 1000)}\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        }\n",
        "\n",
        "    predictions = []\n",
        "    for input_data in input_data_list:\n",
        "        result = predict_delivery_time(input_data)\n",
        "        predictions.append(result[\"predictions\"][0] if result[\"success\"] else None)\n",
        "\n",
        "    return {\n",
        "        \"success\": True,\n",
        "        \"error\": None,\n",
        "        \"predictions\": predictions,\n",
        "        \"request_id\": f\"req_{int(datetime.utcnow().timestamp() * 1000)}\",\n",
        "        \"timestamp\": datetime.utcnow().isoformat()\n",
        "    }\n",
        "'''\n",
        "# Overwrite the file\n",
        "with open(\"api_app.py\", \"w\") as f:\n",
        "    f.write(api_app_code)\n",
        "\n",
        "print(\"‚úÖ api_app.py has been successfully updated.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hsmUU9SrIJv",
        "outputId": "a3814528-1ae8-49c0-ea2b-e0c83a54a5d3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ api_app.py has been successfully updated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib import reload\n",
        "import api_app\n",
        "reload(api_app)\n",
        "\n",
        "from api_app import app\n",
        "from fastapi.testclient import TestClient\n",
        "\n",
        "client = TestClient(app)\n",
        "\n",
        "# ‚úÖ Sample input\n",
        "sample_input = {\n",
        "    \"shipment_id\": \"SHIP12345\",\n",
        "    \"origin\": \"Warehouse A\",\n",
        "    \"destination\": \"City B\",\n",
        "    \"distance_km\": 250.0,\n",
        "    \"shipment_type\": \"Standard\",\n",
        "    \"weight_kg\": 20.5,\n",
        "    \"creation_time\": \"2023-07-27 10:30:00\"\n",
        "}\n",
        "\n",
        "# Endpoint testing again\n",
        "print(\"‚úÖ Testing endpoints after fix:\")\n",
        "print(\"‚úÖ /health:\", client.get(\"/health\").json())\n",
        "print(\"‚úÖ /predict:\", client.post(\"/predict\", json=sample_input).json())\n",
        "print(\"‚úÖ /predict/batch:\", client.post(\"/predict/batch\", json=[sample_input]*5).json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ezeyd4YprK_u",
        "outputId": "239c37d2-daa0-4e64-b3d5-7e06d3e7f27d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Testing endpoints after fix:\n",
            "‚úÖ /health: {'status': 'error', 'message': 'Model not loaded'}\n",
            "‚úÖ /predict: {'success': False, 'error': 'Model not loaded. Please check service status.', 'request_id': 'req_1753633045616', 'timestamp': '2025-07-27T16:17:25.616312'}\n",
            "‚úÖ /predict/batch: {'detail': [{'type': 'missing', 'loc': ['body', 'input_data_list'], 'msg': 'Field required', 'input': None}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually load model and inject it into api_app for notebook testing\n",
        "from inference_pipeline import load_model\n",
        "import api_app\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model()\n",
        "\n",
        "# Inject into FastAPI app manually\n",
        "api_app.model = loaded_model\n",
        "print(\"‚úÖ Model injected into api_app manually.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHnCncT-rc-W",
        "outputId": "c08fbdc7-16e6-4ed1-bc0e-f0ea64bf7dce"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model injected into api_app manually.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "\n",
        "def call_predict():\n",
        "    response = client.post(\"/predict\", json=sample_input)\n",
        "    if response.status_code == 200 and response.json().get(\"success\"):\n",
        "        print(\"‚úÖ\", response.json()[\"predictions\"])\n",
        "    else:\n",
        "        print(\"‚ùå\", response.status_code, response.json().get(\"error\"))\n",
        "\n",
        "threads = []\n",
        "print(\"üöÄ Running 10 parallel predictions...\")\n",
        "\n",
        "for _ in range(10):\n",
        "    t = threading.Thread(target=call_predict)\n",
        "    t.start()\n",
        "    threads.append(t)\n",
        "\n",
        "for t in threads:\n",
        "    t.join()\n",
        "\n",
        "print(\"üéØ Load testing done.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYfoJD6Nrf9z",
        "outputId": "45b37d95-a2d9-45df-c910-1ead4e3df5df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Running 10 parallel predictions...\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "‚úÖ [39.69167709350586]\n",
            "üéØ Load testing done.\n"
          ]
        }
      ]
    }
  ]
}